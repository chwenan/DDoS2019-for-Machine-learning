{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24258ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical #此套件不能用字串方式使用\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dropout,Dense, LSTM\n",
    "from sklearn.metrics import confusion_matrix ,precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import regularizers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_all(df_all, number_feature=4, timesteps=10, d1= 0.3 ,d2 = 0.1, d3 = 0.1):\n",
    "    if number_feature not in (20, 15, 10, 5, 4):\n",
    "        if number_feature > 20:\n",
    "            print(f\"警告：number_feature={number_feature} 不合法，已自動設定為 20\")\n",
    "            number_feature = 20\n",
    "        elif number_feature <= 4:\n",
    "            print(f\"警告：number_feature={number_feature} 不合法，已自動設定為 4\")\n",
    "            number_feature = 4\n",
    "\n",
    "    #1\n",
    "    all_feature = df_all.drop(['Label'],axis = 1)\n",
    "    all_labels = df_all['Label']\n",
    "    #2\n",
    "    le = LabelEncoder()\n",
    "    all_label_tran = le.fit_transform(all_labels)\n",
    "    feature_name = ['Source IP', 'Destination IP', 'Timestamp', 'Flow ID','SimillarHTTP']\n",
    "    for feature in feature_name: \n",
    "        all_feature[feature] = LabelEncoder().fit_transform(all_feature[feature])\n",
    "    # 3\n",
    "    feature_name_number = [\"Destination Port\", \"Flow ID\", \"Source Port\", \"Timestamp\", \"Flow Bytes/s\",\n",
    "                            \"Fwd Seg Size Min\", \"Fwd Packets Length Total\", \"Flow Duration\", \"Flow IAT Min\", \"Fwd Packet Length Max\", \n",
    "                            \"Packet Length Min\", \"Packet Length Max\", \"Flow IAT Std\", \"Fwd IAT Std\", \"Fwd Packet Length Min\", \n",
    "                            \"Avg Packet Size\", \"Flow IAT Max\", \"Fwd Packet Length Mean\", \"Fwd IAT Min\", \"Flow Packets/s\"]\n",
    "    \n",
    "    selected_features = all_feature[feature_name_number[:number_feature]]\n",
    "    #4\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_features = pd.DataFrame(scaler.fit_transform(selected_features),\n",
    "        columns=selected_features.columns)\n",
    "    #5\n",
    "    train_label_onehot = to_categorical(all_label_tran)\n",
    "    #6\n",
    "    lstm_feature=[]\n",
    "    lstm_label=[]\n",
    "    for i in range(len(normalized_features)-timesteps):\n",
    "        lstm_feature.append(normalized_features[i:(i+timesteps)])\n",
    "        lstm_label.append(train_label_onehot[(i+timesteps)])\n",
    "        #print(i)\n",
    "    reshaped_feature = np.array(lstm_feature)\n",
    "    reshaped_label = np.array(lstm_label)\n",
    "    #7\n",
    "    x_train, x_test, y_train, y_test = train_test_split(reshaped_feature,reshaped_label, test_size=0.1,random_state=85)\n",
    "    #8\n",
    "    starttime = time.time()\n",
    "    #LSTM模型建立\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_shape=(timesteps, number_feature),units=256, unroll=False,\n",
    "                    kernel_initializer='glorot_normal', activation='tanh',recurrent_dropout=0.0))\n",
    "    #建立拋棄層\n",
    "    model.add(Dropout(d1))\n",
    "    model.add(Dense(units=128, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(d2))\n",
    "    model.add(Dense(units=64, activation='relu',kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(d3))\n",
    "    model.add(Dense(units=32, activation='relu',kernel_initializer='he_normal'))\n",
    "    #建立輸出層\n",
    "    model.add(Dense(units=12, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "    #9\n",
    "    #訓練方式\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    #進行訓練\n",
    "    train_history = model.fit(x_train, y_train, batch_size=2000, epochs=30, validation_split = 0.1, verbose =1)\n",
    "    #10\n",
    "    predict = model.predict(x_test)\n",
    "    timestop = time.time() - starttime\n",
    "\n",
    "    #11\n",
    "    loss = model.evaluate(x_test, y_test)\n",
    "    #\n",
    "    y_true = np.argmax(y_test, axis = 1)\n",
    "    y_pred = np.argmax(predict, axis = 1)\n",
    "    method = {\n",
    "        \"accuracy\" : accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"f1\": f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "    #12\n",
    "    return train_history, y_test, predict, model, timestop, method, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_parquet('D:/碩士機器學習/data/特徵87/600000數據實驗/df_all2.parquet')\n",
    "results = []\n",
    "number_features_option = [20, 15, 10, 5, 4] \n",
    "timestep_option = [15, 10, 5] \n",
    "total = len(number_features_option)*len(timestep_option)\n",
    "for conut ,(number_features, timestep) in enumerate(product(number_features_option,timestep_option), start= 1):\n",
    "    print(f\"{conut} / {total}:featues = {number_features}, timesteps = {timestep}\")\n",
    "    train_history, y_test, predict, model, timestop, method, loss = LSTM_all(d,number_feature=number_features, timesteps= timestep)\n",
    "    results.append({\n",
    "    \"features\": number_features,\n",
    "    \"timesteps\": timestep,\n",
    "    \"train_history\": train_history,\n",
    "    \"y_test\": y_test,\n",
    "    \"predict\": predict,\n",
    "    \"loss\":loss,\n",
    "    \"model\": model,\n",
    "    \"timestop\": timestop,\n",
    "    \"method\": method,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "     print(f\"第{i+1}筆訓練時間: {results[i]['timestop']:.1f} 秒 {results[i]['features']}F-{results[i]['timesteps']}T {results[i]['loss'][0]*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#折線圖準確率與驗證準確率\n",
    "rcParams['font.family'] = 'Microsoft JhengHei'\n",
    "for i in range(len(results)):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f\"第{i+1}組參數 Train History {results[i]['features']}F-{results[i]['timesteps']}T\")\n",
    "    plt.plot(results[i][\"train_history\"].history['accuracy'],marker = \"o\")\n",
    "    plt.plot(results[i][\"train_history\"].history['val_accuracy'],marker = \"o\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend([\"acc \",\"val_acc\"])\n",
    "    plt.grid(True)\n",
    "    #plt.yticks(ticks=[i/100 for i in range(50, 101,5)])\n",
    "    plt.show()\n",
    "    #filename = f\"photo/recurrent_dropout/第{i+1}次{results[i]['features']}F_{results[i]['timesteps']}T_Acc.png\"\n",
    "    #plt.savefig(filename, dpi=300, bbox_inches='tight')  # 儲存成高解析度圖檔\n",
    "\n",
    "#折線圖損失值與驗證損失值\n",
    "for i in range(len(results)):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f\"第{i+1}組參數 Train History {results[i]['features']}F-{results[i]['timesteps']}T\")\n",
    "    plt.plot(results[i][\"train_history\"].history['loss'],marker = \"o\")\n",
    "    plt.plot(results[i][\"train_history\"].history['val_loss'],marker = \"o\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend([\"loss \",\"val_loss\"])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #filename = f\"photo/recurrent_dropout/第{i+1}次{results[i]['features']}F_{results[i]['timesteps']}T_Loss.png\"\n",
    "    #plt.savefig(filename, dpi=300, bbox_inches='tight')  # 儲存成高解析度圖檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axes = plt.subplots(3, 5, figsize=(20, 12))  # 3 rows x 5 columns\n",
    "fig1.suptitle(\"Train History of All 15 Acc\" , fontsize=20)\n",
    "for i in range(len(results)):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    ax = axes[row][col]\n",
    "\n",
    "    history = results[i][\"train_history\"].history\n",
    "    label = f\"{results[i]['features']}F-{results[i]['timesteps']}T\"\n",
    "\n",
    "    if  'accuracy' in history and 'val_accuracy' in history:\n",
    "        ax.plot(history['accuracy'], marker='o', label=\"acc\")\n",
    "        ax.plot(history['val_accuracy'], marker='o', label=\"val_acc\")\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.grid(True)\n",
    "        ax.legend(fontsize=\"small\") \n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # 預留空間給大標題\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axes = plt.subplots(3, 5, figsize=(20, 12))  # 3 rows x 5 columns\n",
    "fig2.suptitle(\"Train History of All 15 Loss recurrent_dropout=0.5\", fontsize=20)\n",
    "for i in range(len(results)):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    ax = axes[row][col]\n",
    "\n",
    "    history = results[i][\"train_history\"].history\n",
    "    label = f\"{results[i]['features']}F-{results[i]['timesteps']}T\"\n",
    "\n",
    "    if 'loss' in history and 'val_loss' in history:\n",
    "        ax.plot(history['loss'], marker='o', label=\"loss\")\n",
    "        ax.plot(history['val_loss'], marker='o', label=\"val_loss\")\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.grid(True)\n",
    "        ax.legend(fontsize=\"small\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # 預留空間給大標題\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    print(f\"第{i+1}組參數{results[i]['features']}F-{results[i]['timesteps']}T\")\n",
    "    print(\"Accuracy: %.3f%%\" % (results[i]['method'][\"accuracy\"] * 100.0))\n",
    "    print(\"Precision: %.3f%%\" % (results[i]['method'][\"precision\"] * 100.0))\n",
    "    print(\"Recall: %.3f%%\" % (results[i]['method'][\"recall\"] * 100.0))\n",
    "    print(\"F1-score: %.3f%%\" % (results[i]['method'][\"f1\"] * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    cm=confusion_matrix(np.argmax(results[i]['y_test'],axis=1), np.argmax(results[i]['predict'],axis=1))\n",
    "    cm_label = ['Benign', 'DrDoS_DNS', 'DrDoS_LDAP', 'DrDoS_MSSQL', 'DrDoS_NTP', 'DrDoS_NetBIOS', 'DrDoS_SNMP', 'DrDoS_SSDP', 'DrDoS_UDP', 'Syn', 'TFTP', 'UDP-lag']\n",
    "    plt.figure(figsize=(16, 13))\n",
    "    cms = sns.heatmap(cm, annot=True, cmap='Blues', fmt='d',annot_kws={\"size\": 10, \"verticalalignment\": \"center\", \"horizontalalignment\": \"center\"},vmin=0, vmax=10000)\n",
    "    plt.xlabel('Y_test labels',fontsize=20)\n",
    "    plt.ylabel('Predicted labels',rotation=90,fontsize=20)\n",
    "    plt.title(f\"第{i+1}組參數的Confusion Matrix {results[i]['features']}F-{results[i]['timesteps']}T recurrent_dropout=0.8\",fontsize=20)\n",
    "    cms.set_xticklabels(cm_label, rotation=45, ha='right',fontsize=14)\n",
    "    cms.set_yticklabels(cm_label, rotation=0,fontsize=14)\n",
    "    plt.show()\n",
    "    #filename = f\"confusion Matrix photo/第{i+1}次{results[i]['features']}F_{results[i]['timesteps']}T_confusion Matrix.png\"\n",
    "    #plt.savefig(filename, dpi=300, bbox_inches='tight')  # 儲存成高解析度圖檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec14f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精確度、召回率、f1-score分數表格\n",
    "for i in range(len(results)):\n",
    "    print(f\"\\n 第{i +1}次 precision、recall、f1詳細資訊 {results[i]['features']}F-{results[i]['timesteps']}T\")\n",
    "    target_names=[\"Benign\", \"DrDoS_DNS\", \"DrDoS_LDAP\", \"DrDoS_MSSQL\", \"DrDoS_NTP\", \"DrDoS_NetBIOS\", \"DrDoS_SNMP\", \"DrDoS_SSDP\", \"DrDoS_UDP\", \"Syn\", \"TFTP\", \"UDP-lag\"]\n",
    "    classification = classification_report(np.argmax(results[i]['y_test'],axis=1), np.argmax(results[i]['predict'],axis=1),target_names=target_names)\n",
    "    print(classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
